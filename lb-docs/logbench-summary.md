# LogBench: Automated Logging Generation Benchmark - Concise Summary

## Objective

This paper investigates the effectiveness of Large Language Models (LLMs) in automated logging statement generation, a critical yet under-researched aspect of software engineering. The study aims to evaluate whether modern LLMs can assist developers in writing high-quality logging statements by automatically generating three key ingredients: logging levels (error, warn, info, debug, trace), logging variables (runtime state information), and logging texts (natural language descriptions). To address this, the authors introduce LogBench, a comprehensive benchmark consisting of two datasets—LogBench-O (original real-world logging statements) and LogBench-T (semantically-equivalent transformed code for testing generalization)—and evaluate 11 state-of-the-art LLMs against conventional specialized models to determine their capabilities and limitations in this practical software engineering task.

## Dataset Construction

The LogBench dataset was constructed by mining 3,089 high-quality Java repositories from GitHub, selected based on strict criteria (20+ stars, 100+ commits, 5+ contributors) to ensure professional logging practices. The researchers extracted 6,849 logging statements from 3,870 methods across 2,430 Java files using regular expression matching for Log4j and SLF4J frameworks. Each benchmark sample was created by removing a logging statement from its original code context and using it as ground truth for evaluation. To address data leakage concerns—where LLMs may have seen the original code during training—the authors developed LogBench-T using eight carefully engineered AST-level code transformers (including condition manipulation, variable extraction, loop conversion, and parenthesis insertion) that generate semantically-equivalent but syntactically different code variants. This transformation approach preserves code readability and functionality while creating truly unseen test cases, with the transformation tool implemented in 4,074 lines of Java code using the JavaParser library and verified through unit testing.

## Results

The evaluation revealed that while LLMs show promise in automated logging generation, significant challenges remain, particularly in generating high-quality logging texts. GitHub Copilot achieved the best overall performance with 74.3% accuracy for logging level prediction and 71.2% F1 score for variable identification, but only 0.244 BLEU-4 score (24.4% n-gram overlap) for logging text generation, indicating that natural language generation in technical contexts remains difficult. Notably, all LLMs outperformed conventional specialized models without task-specific fine-tuning, demonstrating effective transfer learning from general coding knowledge. However, a critical finding emerged from LogBench-T evaluation: models experienced an 8.2%-16.2% performance drop on semantically-equivalent transformed code, suggesting potential memorization rather than pure reasoning. The study also found that providing file-level context instead of method-level context dramatically improved text generation performance by 49.3% (BLEU-4), while removing code comments resulted in 2-3% performance degradation across metrics, highlighting the importance of broader context and natural language hints for effective logging generation.
