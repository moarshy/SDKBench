{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK-Bench: Step-by-Step Evaluation Pipeline\n",
    "\n",
    "This notebook demonstrates the complete SDK-Bench evaluation pipeline, showing:\n",
    "1. How prompts are constructed\n",
    "2. What each metric measures\n",
    "3. How evaluation works\n",
    "4. Actual results on a sample task\n",
    "\n",
    "We'll use a real sample from the SDK-Bench dataset and walk through every stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import SDK-Bench modules\n",
    "from sdkbench.llm import LLMConfig, AnthropicProvider\n",
    "from sdkbench.llm.prompt_builder import PromptBuilder\n",
    "from sdkbench.llm.solution_generator import SolutionGenerator\n",
    "from sdkbench.evaluator import Evaluator\n",
    "from sdkbench.core.ground_truth import GroundTruth\n",
    "\n",
    "# Note: The actual metric evaluator classes are imported but not used directly\n",
    "# The Evaluator class handles all metric evaluations internally\n",
    "# If you need to use them directly, import like this:\n",
    "# from sdkbench.metrics import (\n",
    "#     IAccEvaluator, CCompEvaluator, IPAEvaluator,\n",
    "#     FCorrEvaluator, CQEvaluator, SemSimEvaluator\n",
    "# )\n",
    "\n",
    "# For pretty printing\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/Users/arshath/play/naptha/better-onboarding/SDKBench/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a Sample Task\n",
    "\n",
    "Let's load a sample task and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Task: task1_init_001\n",
      "\n",
      "Task Metadata:\n",
      "{ 'clerk_version': '5.0.0',\n",
      "  'description': 'Initialize Clerk authentication by wrapping the application '\n",
      "                 'with ClerkProvider',\n",
      "  'difficulty': 'easy',\n",
      "  'estimated_lines': 10,\n",
      "  'evaluation_targets': { 'c_comp': { 'optional_env_vars': 0,\n",
      "                                      'required_env_vars': 2},\n",
      "                          'f_corr': { 'expected_pass': True,\n",
      "                                      'test_command': 'npm test -- '\n",
      "                                                      'init.test.ts'},\n",
      "                          'i_acc': { 'correct_file': 'app/layout.tsx',\n",
      "                                     'correct_imports': [ 'ClerkProvider from '\n",
      "                                                          '@clerk/nextjs'],\n",
      "                                     'correct_pattern': 'ClerkProvider'}},\n",
      "  'framework': 'nextjs',\n",
      "  'ground_truth': { 'ingredients': { 'configuration': { 'env_vars': [ 'NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY',\n",
      "                                                                      'CLERK_SECRET_KEY'],\n",
      "                                                        'optional_config': [],\n",
      "                                                        'provider_props': []},\n",
      "                                     'initialization': { 'imports': [ '@clerk/nextjs'],\n",
      "                                                         'location': 'app/layout.tsx',\n",
      "                                                         'pattern': 'ClerkProvider '\n",
      "                                                                    'wrapper'}}},\n",
      "  'sample_id': 'task1_init_001',\n",
      "  'task_name': 'initialization',\n",
      "  'task_type': 1}\n"
     ]
    }
   ],
   "source": [
    "# Choose a sample task\n",
    "SAMPLE_ID = \"task1_init_001\"  # Basic initialization task\n",
    "sample_path = Path(\"../samples\") / SAMPLE_ID\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = sample_path / \"expected\" / \"metadata.json\"\n",
    "with open(metadata_path) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Sample Task:\", SAMPLE_ID)\n",
    "print(\"\\nTask Metadata:\")\n",
    "pp.pprint(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Task Types\n",
    "\n",
    "SDK-Bench has 6 task types that test different aspects of SDK instrumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Sample Task Type: 1\n",
      "Name: Initialization & Configuration\n",
      "Description: Basic SDK setup and initialization\n",
      "\n",
      "Key Challenges:\n",
      "  - Correct import statements\n",
      "  - Proper component wrapping\n",
      "  - Environment variable configuration\n"
     ]
    }
   ],
   "source": [
    "task_types = {\n",
    "    1: {\n",
    "        \"name\": \"Initialization & Configuration\",\n",
    "        \"description\": \"Basic SDK setup and initialization\",\n",
    "        \"example\": \"Add Clerk authentication to a Next.js app\",\n",
    "        \"key_challenges\": [\n",
    "            \"Correct import statements\",\n",
    "            \"Proper component wrapping\",\n",
    "            \"Environment variable configuration\"\n",
    "        ]\n",
    "    },\n",
    "    2: {\n",
    "        \"name\": \"Basic Feature Integration\",\n",
    "        \"description\": \"Implement core SDK features\",\n",
    "        \"example\": \"Add sign-in/sign-out buttons\",\n",
    "        \"key_challenges\": [\n",
    "            \"Using correct SDK hooks\",\n",
    "            \"Proper UI component placement\",\n",
    "            \"State management\"\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        \"name\": \"Advanced Feature Integration\",\n",
    "        \"description\": \"Complex SDK feature implementation\",\n",
    "        \"example\": \"Multi-factor authentication or custom user metadata\",\n",
    "        \"key_challenges\": [\n",
    "            \"Complex API interactions\",\n",
    "            \"Error handling\",\n",
    "            \"Advanced configuration\"\n",
    "        ]\n",
    "    },\n",
    "    4: {\n",
    "        \"name\": \"Debugging & Troubleshooting\",\n",
    "        \"description\": \"Fix broken SDK implementations\",\n",
    "        \"example\": \"Fix authentication flow that's not working\",\n",
    "        \"key_challenges\": [\n",
    "            \"Identifying the bug\",\n",
    "            \"Understanding error messages\",\n",
    "            \"Applying correct fix\"\n",
    "        ]\n",
    "    },\n",
    "    5: {\n",
    "        \"name\": \"Migration\",\n",
    "        \"description\": \"Upgrade SDK to newer version\",\n",
    "        \"example\": \"Migrate from Clerk v4 to v5\",\n",
    "        \"key_challenges\": [\n",
    "            \"API breaking changes\",\n",
    "            \"Deprecated methods\",\n",
    "            \"New patterns adoption\"\n",
    "        ]\n",
    "    },\n",
    "    6: {\n",
    "        \"name\": \"Performance Optimization\",\n",
    "        \"description\": \"Optimize SDK usage for better performance\",\n",
    "        \"example\": \"Reduce bundle size or optimize API calls\",\n",
    "        \"key_challenges\": [\n",
    "            \"Identifying bottlenecks\",\n",
    "            \"Applying optimizations\",\n",
    "            \"Maintaining functionality\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "current_task_type = metadata['task_type']\n",
    "print(f\"Current Sample Task Type: {current_task_type}\")\n",
    "print(f\"Name: {task_types[current_task_type]['name']}\")\n",
    "print(f\"Description: {task_types[current_task_type]['description']}\")\n",
    "print(f\"\\nKey Challenges:\")\n",
    "for challenge in task_types[current_task_type]['key_challenges']:\n",
    "    print(f\"  - {challenge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Prompt\n",
    "\n",
    "Now let's see how the prompt is constructed for the LLM. The prompt has two parts:\n",
    "1. **System Prompt**: Sets the context and role\n",
    "2. **User Prompt**: Provides the specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SYSTEM PROMPT\n",
      "================================================================================\n",
      "You are an expert developer specializing in authentication integration.\n",
      "You are helping integrate Clerk authentication (version 5.0.0) into a nextjs application.\n",
      "\n",
      "\n",
      "Clerk is a complete authentication and user management solution for modern web applications.\n",
      "\n",
      "Key Concepts:\n",
      "1. ClerkProvider: Wraps your React app to provide authentication context\n",
      "2. Middleware: Protects routes on the server-side\n",
      "3. Hooks: Access user data and auth state in React components\n",
      "4. Server-side helpers: Get auth state in server components and API routes\n",
      "\n",
      "Clerk v5 (Latest):\n",
      "- Package: @clerk/nextjs\n",
      "- Middleware: clerkMiddleware()\n",
      "- Server imports: @clerk/nextjs/server\n",
      "- Client imports: @clerk/nextjs\n",
      "\n",
      "Clerk v4 (Legacy):\n",
      "- Package: @clerk/nextjs@4\n",
      "- Middleware: authMiddleware()\n",
      "- Different import paths\n",
      "\n",
      "Environment Variables:\n",
      "- NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY: Your publishable key\n",
      "- CLERK_SECRET_KEY: Your secret key\n",
      "- Optional URL configs for custom sign-in/up pages\n",
      "\n",
      "\n",
      "Your responses should:\n",
      "1. Provide working code that follows best practices\n",
      "2. Include all necessary imports\n",
      "3. Add appropriate error handling\n",
      "4. Follow the framework's conventions\n",
      "5. Be production-ready\n",
      "\n",
      "When providing code, always specify the file path as a comment at the top of each code block.\n",
      "Format: // filepath: path/to/file.ext\n",
      "\n",
      "\n",
      "================================================================================\n",
      "USER PROMPT\n",
      "================================================================================\n",
      "Task: Initialization\n",
      "Initialize Clerk authentication by wrapping the application with ClerkProvider\n",
      "\n",
      "Current project files:\n",
      "\n",
      "=== package.json ===\n",
      "```\n",
      "{\n",
      "  \"name\": \"nextjs-app\",\n",
      "  \"version\": \"0.1.0\",\n",
      "  \"private\": true,\n",
      "  \"scripts\": {\n",
      "    \"dev\": \"next dev\",\n",
      "    \"build\": \"next build\",\n",
      "    \"start\": \"next start\"\n",
      "  },\n",
      "  \"dependencies\": {\n",
      "    \"react\": \"^18.2.0\",\n",
      "    \"react-dom\": \"^18.2.0\",\n",
      "    \"next\": \"^14.0.0\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "=== .env.example ===\n",
      "```\n",
      "# Add environment variables here\n",
      "\n",
      "```\n",
      "\n",
      "=== app/layout.tsx ===\n",
      "```\n",
      "export default function RootLayout({\n",
      "  children,\n",
      "}: {\n",
      "  children: React.ReactNode\n",
      "}) {\n",
      "  return (\n",
      "    <html lang=\"en\">\n",
      "      <body>\n",
      "        {/* TODO: Add Clerk authentication provider */}\n",
      "        {children}\n",
      "      </body>\n",
      "    </html>\n",
      "  )\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "=== app/page.tsx ===\n",
      "```\n",
      "export default function Home() {\n",
      "  return (\n",
      "    <main>\n",
      "      <h1>Welcome</h1>\n",
      "      <p>This app needs Clerk authentication.</p>\n",
      "    </main>\n",
      "  )\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "For this initialization task:\n",
      "1. Add ClerkProvider to wrap the application\n",
      "2. Import necessary Clerk components\n",
      "3. Ensure proper placement in component hierarchy\n",
      "4. Add any required configuration\n",
      "\n",
      "Please provide the complete solution with all necessary files and configurations.\n",
      "Mark each file clearly with its path using the format: // filepath: path/to/file.ext\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the prompt\n",
    "builder = PromptBuilder()\n",
    "input_dir = sample_path / \"input\"\n",
    "\n",
    "system_prompt, user_prompt = builder.build_from_metadata(metadata_path, input_dir)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEM PROMPT\")\n",
    "print(\"=\"*80)\n",
    "print(system_prompt)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"USER PROMPT\")\n",
    "print(\"=\"*80)\n",
    "print(user_prompt[:2000] + \"...\" if len(user_prompt) > 2000 else user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding the Prompt Structure\n",
    "\n",
    "Let's break down what's included in the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Components:\n",
      "  - Length: 1290 characters\n",
      "  - Sets role as SDK integration expert\n",
      "  - Provides response format instructions\n",
      "  - Specifies file output format\n",
      "\n",
      "User Prompt Components:\n",
      "  - Length: 1296 characters\n",
      "  - Task description: Initialize Clerk authentication by wrapping the application with ClerkProvider...\n",
      "  - SDK: Clerk v5.0.0\n",
      "  - Framework: nextjs\n",
      "  - Number of input files: 5\n",
      "\n",
      "Input Files Provided:\n",
      "  - package.json (262 bytes)\n",
      "  - .env.example (33 bytes)\n",
      "  - app/layout.tsx (239 bytes)\n",
      "  - app/page.tsx (146 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Analyze prompt components\n",
    "print(\"System Prompt Components:\")\n",
    "print(f\"  - Length: {len(system_prompt)} characters\")\n",
    "print(f\"  - Sets role as SDK integration expert\")\n",
    "print(f\"  - Provides response format instructions\")\n",
    "print(f\"  - Specifies file output format\")\n",
    "\n",
    "print(\"\\nUser Prompt Components:\")\n",
    "print(f\"  - Length: {len(user_prompt)} characters\")\n",
    "print(f\"  - Task description: {metadata['description'][:100]}...\")\n",
    "print(f\"  - SDK: {metadata.get('sdk_name', 'Clerk')} v{metadata['clerk_version']}\")\n",
    "print(f\"  - Framework: {metadata['framework']}\")\n",
    "print(f\"  - Number of input files: {len(list(input_dir.glob('**/*'))) if input_dir.exists() else 0}\")\n",
    "\n",
    "# Show input files if they exist\n",
    "if input_dir.exists():\n",
    "    print(\"\\nInput Files Provided:\")\n",
    "    for file in input_dir.rglob('*'):\n",
    "        if file.is_file():\n",
    "            rel_path = file.relative_to(input_dir)\n",
    "            size = file.stat().st_size\n",
    "            print(f\"  - {rel_path} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Solution (Optional - Uses LLM)\n",
    "\n",
    "You can either:\n",
    "1. Use an existing solution from previous runs\n",
    "2. Generate a new solution using the LLM (requires API key)\n",
    "\n",
    "For demonstration, we'll show both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing solution from: ../results/llm_solutions/task1_init_001/claude-3-haiku-20240307\n",
      "\n",
      "Solution Files:\n",
      "\n",
      ".env.local:\n",
      "----------------------------------------\n",
      "NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=your_clerk_publishable_key\n",
      "CLERK_SECRET_KEY=your_clerk_secret_key\n",
      "\n",
      "package.json:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"name\": \"nextjs-app\",\n",
      "  \"version\": \"0.1.0\",\n",
      "  \"private\": true,\n",
      "  \"scripts\": {\n",
      "    \"dev\": \"next dev\",\n",
      "    \"build\": \"next build\",\n",
      "    \"start\": \"next start\"\n",
      "  },\n",
      "  \"dependencies\": {\n",
      "    \"react\": \"^18.2.0\",\n",
      "    \"react-dom\": \"^18.2.0\",\n",
      "    \"next\": \"^14.0.0\"\n",
      "  }\n",
      "}\n",
      "\n",
      "generation_metadata.json:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"sample_id\": \"task1_init_001\",\n",
      "  \"model\": \"claude-3-haiku-20240307\",\n",
      "  \"generated_at\": \"2025-11-23T20:59:25.597304\",\n",
      "  \"files_generated\": [\n",
      "    \".env.local\",\n",
      "    \"package.json\",\n",
      "    \".env.example\",\n",
      "    \"llm_response.txt\",\n",
      "    \"app/layout.tsx\",\n",
      "    \"app/page.tsx\"\n",
      "  ]\n",
      "}\n",
      "\n",
      ".env.example:\n",
      "----------------------------------------\n",
      "# Add environment variables here\n",
      "\n",
      "\n",
      "app/layout.tsx:\n",
      "----------------------------------------\n",
      "import { ClerkProvider } from \"@clerk/nextjs\";\n",
      "\n",
      "export default function RootLayout({\n",
      "  children,\n",
      "}: {\n",
      "  children: React.ReactNode\n",
      "}) {\n",
      "  return (\n",
      "    <html lang=\"en\">\n",
      "      <body>\n",
      "        <ClerkProvider>\n",
      "          {children}\n",
      "        </ClerkProvider>\n",
      "      </body>\n",
      "    </html>\n",
      "  )\n",
      "}\n",
      "\n",
      "app/page.tsx:\n",
      "----------------------------------------\n",
      "export default function Home() {\n",
      "  return (\n",
      "    <main>\n",
      "      <h1>Welcome</h1>\n",
      "      <p>This app needs Clerk authentication.</p>\n",
      "    </main>\n",
      "  )\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use existing solution\n",
    "USE_EXISTING = True\n",
    "existing_solution_dir = Path(\"../results/llm_solutions\") / SAMPLE_ID / \"claude-3-haiku-20240307\"\n",
    "\n",
    "if USE_EXISTING and existing_solution_dir.exists():\n",
    "    print(\"Using existing solution from:\", existing_solution_dir)\n",
    "    solution_dir = existing_solution_dir\n",
    "    \n",
    "    # Show solution files\n",
    "    print(\"\\nSolution Files:\")\n",
    "    for file in solution_dir.rglob('*'):\n",
    "        if file.is_file() and file.name not in ['metadata.json', 'llm_response.txt']:\n",
    "            rel_path = file.relative_to(solution_dir)\n",
    "            print(f\"\\n{rel_path}:\")\n",
    "            print(\"-\" * 40)\n",
    "            with open(file) as f:\n",
    "                content = f.read()\n",
    "                print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "else:\n",
    "    print(\"To generate a new solution, set USE_EXISTING=False and provide API key\")\n",
    "    print(\"Example:\")\n",
    "    print(\"  os.environ['ANTHROPIC_API_KEY'] = 'your-key'\")\n",
    "    print(\"  # Then run the generation code below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "  - Model: claude-3-haiku-20240307\n",
      "  - Tokens: 1331\n",
      "  - Cost: $0.0009\n",
      "  - Latency: 4538ms\n",
      "\n",
      "Solution generated at: ../temp_solutions/task1_init_001/claude-3-haiku-20240307\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Generate new solution (requires API key)\n",
    "GENERATE_NEW = True  # Set to True to generate\n",
    "\n",
    "if GENERATE_NEW:\n",
    "    # Check for API key\n",
    "    if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "        print(\"Please set ANTHROPIC_API_KEY environment variable\")\n",
    "    else:\n",
    "        # Configure LLM\n",
    "        config = LLMConfig(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000,\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY')\n",
    "        )\n",
    "        \n",
    "        # Create provider and generate\n",
    "        provider = AnthropicProvider(config)\n",
    "        response = provider.generate(user_prompt, system_prompt)\n",
    "        \n",
    "        print(\"LLM Response:\")\n",
    "        print(f\"  - Model: {response.model}\")\n",
    "        print(f\"  - Tokens: {response.tokens_used}\")\n",
    "        print(f\"  - Cost: ${response.cost:.4f}\")\n",
    "        print(f\"  - Latency: {response.latency_ms:.0f}ms\")\n",
    "        \n",
    "        # Generate solution files\n",
    "        generator = SolutionGenerator()\n",
    "        solution_dir = generator.generate_solution(\n",
    "            response.content,\n",
    "            Path(\"../temp_solutions\"),\n",
    "            SAMPLE_ID,\n",
    "            config.model,\n",
    "            copy_input=input_dir if input_dir.exists() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSolution generated at: {solution_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To initialize Clerk authentication in your NextJS application, follow these steps:\n",
      "\n",
      "// filepath: app/layout.tsx\n",
      "```typescript\n",
      "import { ClerkProvider } from \"@clerk/nextjs\";\n",
      "\n",
      "export default function RootLayout({\n",
      "  children,\n",
      "}: {\n",
      "  children: React.ReactNode\n",
      "}) {\n",
      "  return (\n",
      "    <html lang=\"en\">\n",
      "      <body>\n",
      "        <ClerkProvider>\n",
      "          {children}\n",
      "        </ClerkProvider>\n",
      "      </body>\n",
      "    </html>\n",
      "  )\n",
      "}\n",
      "```\n",
      "\n",
      "In this updated `app/layout.tsx` file, we have wrapped the entire application with the `ClerkProvider` component. This ensures that the Clerk authentication context is available throughout the application.\n",
      "\n",
      "To use Clerk's features, you'll need to configure the necessary environment variables. Create a `.env.local` file in the root of your project and add the following variables:\n",
      "\n",
      "// filepath: .env.local\n",
      "```\n",
      "NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=your_clerk_publishable_key\n",
      "CLERK_SECRET_KEY=your_clerk_secret_key\n",
      "```\n",
      "\n",
      "Replace `your_clerk_publishable_key` and `your_clerk_secret_key` with the actual values from your Clerk dashboard.\n",
      "\n",
      "Now, your NextJS application is initialized with Clerk authentication. You can start using Clerk's features, such as signing in, signing up, and managing user sessions, in your components.\n",
      "\n",
      "Here's an example of how you can use the `useUser` hook in your `app/page.tsx` file:\n",
      "\n",
      "// filepath: app/page.tsx\n",
      "```typescript\n",
      "import { useUser } from \"@clerk/nextjs\";\n",
      "\n",
      "export default function Home() {\n",
      "  const { user, isLoaded } = useUser();\n",
      "\n",
      "  if (!isLoaded) {\n",
      "    return <div>Loading...</div>;\n",
      "  }\n",
      "\n",
      "  return (\n",
      "    <main>\n",
      "      <h1>Welcome</h1>\n",
      "      {user ? (\n",
      "        <p>Logged in as: {user.firstName} {user.lastName}</p>\n",
      "      ) : (\n",
      "        <p>This app needs Clerk authentication.</p>\n",
      "      )}\n",
      "    </main>\n",
      "  );\n",
      "}\n",
      "```\n",
      "\n",
      "In this example, we use the `useUser` hook to access the current user's information and display it on the homepage. If the user is not logged in, we display a message prompting them to authenticate.\n",
      "\n",
      "Remember to handle any potential errors that may occur during the authentication process, such as network failures or invalid credentials. You can use Clerk's error handling utilities or implement your own error handling logic.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "\n",
    "SDK-Bench uses 6 metrics to evaluate the quality of SDK instrumentation. Let's understand each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I-ACC (Implementation Accuracy) (Weight: 30.0%)\n",
      "============================================================\n",
      "Measures: Whether the solution implements the required SDK functionality\n",
      "How: Checks if key SDK methods and components are present\n",
      "Range: 0-100%\n",
      "Example: For Clerk: checks if ClerkProvider is used, authentication hooks are called\n",
      "\n",
      "C-COMP (Configuration Completeness) (Weight: 20.0%)\n",
      "============================================================\n",
      "Measures: Whether all required configuration is present\n",
      "How: Verifies environment variables, config files, and settings\n",
      "Range: 0-100%\n",
      "Example: Checks if .env has CLERK_PUBLISHABLE_KEY and CLERK_SECRET_KEY\n",
      "\n",
      "IPA (Integration Point Accuracy) (Weight: 15.0%)\n",
      "============================================================\n",
      "Measures: Whether SDK is integrated at the correct locations\n",
      "How: Compares file paths and integration points with ground truth\n",
      "Range: 0-1 (F1 score)\n",
      "Example: Checks if authentication is added to the right components\n",
      "\n",
      "F-CORR (Functional Correctness) (Weight: 15.0%)\n",
      "============================================================\n",
      "Measures: Whether the code compiles and runs without errors\n",
      "How: Runs build/test commands and checks for errors\n",
      "Range: 0-100%\n",
      "Example: npm run build && npm test\n",
      "\n",
      "CQ (Code Quality) (Weight: 10.0%)\n",
      "============================================================\n",
      "Measures: Code quality and best practices\n",
      "How: Checks formatting, patterns, error handling\n",
      "Range: 0-100%\n",
      "Example: Proper imports, no unused variables, follows framework patterns\n",
      "\n",
      "SEM-SIM (Semantic Similarity) (Weight: 10.0%)\n",
      "============================================================\n",
      "Measures: How similar the solution is to the reference implementation\n",
      "How: Compares code structure and logic using embeddings/AST\n",
      "Range: 0-100%\n",
      "Example: Similar variable names, function structure, logic flow\n"
     ]
    }
   ],
   "source": [
    "metrics_explanation = {\n",
    "    \"I-ACC (Implementation Accuracy)\": {\n",
    "        \"measures\": \"Whether the solution implements the required SDK functionality\",\n",
    "        \"how\": \"Checks if key SDK methods and components are present\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"example\": \"For Clerk: checks if ClerkProvider is used, authentication hooks are called\",\n",
    "        \"weight\": 0.3\n",
    "    },\n",
    "    \"C-COMP (Configuration Completeness)\": {\n",
    "        \"measures\": \"Whether all required configuration is present\",\n",
    "        \"how\": \"Verifies environment variables, config files, and settings\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"example\": \"Checks if .env has CLERK_PUBLISHABLE_KEY and CLERK_SECRET_KEY\",\n",
    "        \"weight\": 0.2\n",
    "    },\n",
    "    \"IPA (Integration Point Accuracy)\": {\n",
    "        \"measures\": \"Whether SDK is integrated at the correct locations\",\n",
    "        \"how\": \"Compares file paths and integration points with ground truth\",\n",
    "        \"range\": \"0-1 (F1 score)\",\n",
    "        \"example\": \"Checks if authentication is added to the right components\",\n",
    "        \"weight\": 0.15\n",
    "    },\n",
    "    \"F-CORR (Functional Correctness)\": {\n",
    "        \"measures\": \"Whether the code compiles and runs without errors\",\n",
    "        \"how\": \"Runs build/test commands and checks for errors\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"example\": \"npm run build && npm test\",\n",
    "        \"weight\": 0.15\n",
    "    },\n",
    "    \"CQ (Code Quality)\": {\n",
    "        \"measures\": \"Code quality and best practices\",\n",
    "        \"how\": \"Checks formatting, patterns, error handling\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"example\": \"Proper imports, no unused variables, follows framework patterns\",\n",
    "        \"weight\": 0.1\n",
    "    },\n",
    "    \"SEM-SIM (Semantic Similarity)\": {\n",
    "        \"measures\": \"How similar the solution is to the reference implementation\",\n",
    "        \"how\": \"Compares code structure and logic using embeddings/AST\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"example\": \"Similar variable names, function structure, logic flow\",\n",
    "        \"weight\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, details in metrics_explanation.items():\n",
    "    print(f\"\\n{metric} (Weight: {details['weight']*100}%)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Measures: {details['measures']}\")\n",
    "    print(f\"How: {details['how']}\")\n",
    "    print(f\"Range: {details['range']}\")\n",
    "    print(f\"Example: {details['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n",
      "\n",
      "Evaluation Complete!\n",
      "============================================================\n",
      "Overall Score: 33.5%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create evaluator\n",
    "evaluator = Evaluator(solution_dir, metadata_path=metadata_path)\n",
    "\n",
    "# Run quick evaluation (without build/test)\n",
    "print(\"Running evaluation...\")\n",
    "result = evaluator.evaluate_quick()\n",
    "\n",
    "print(\"\\nEvaluation Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Score: {result.overall_score:.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation\n",
    "\n",
    "Now let's evaluate the solution using all metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Metric Results\n",
    "\n",
    "Let's examine each metric's evaluation in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-COMP: Configuration Completeness  \n",
    "print(\"2. C-COMP (Configuration Completeness)\")\n",
    "print(\"=\"*60)\n",
    "if result.c_comp:\n",
    "    print(f\"Score: {result.c_comp.score:.1f}%\")\n",
    "    print(f\"\\nComponent Scores:\")\n",
    "    print(f\"  - Environment Variables: {result.c_comp.env_vars_score*100:.1f}%\")\n",
    "    print(f\"  - Provider Properties: {result.c_comp.provider_props_score*100:.1f}%\")\n",
    "    print(f\"  - Middleware Config: {result.c_comp.middleware_config_score*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nWeighting:\")\n",
    "    print(f\"  - Environment Variables: 50% (contributes {result.c_comp.env_vars_score*50:.1f}%)\")\n",
    "    print(f\"  - Provider Properties: 30% (contributes {result.c_comp.provider_props_score*30:.1f}%)\")\n",
    "    print(f\"  - Middleware Config: 20% (contributes {result.c_comp.middleware_config_score*20:.1f}%)\")\n",
    "    \n",
    "    if result.c_comp.missing_env_vars:\n",
    "        print(f\"\\nMissing Environment Variables:\")\n",
    "        for var in result.c_comp.missing_env_vars:\n",
    "            print(f\"  ✗ {var}\")\n",
    "    \n",
    "    if result.c_comp.missing_provider_props:\n",
    "        print(f\"\\nMissing Provider Properties:\")\n",
    "        for prop in result.c_comp.missing_provider_props:\n",
    "            print(f\"  ✗ {prop}\")\n",
    "            \n",
    "    if result.c_comp.missing_middleware_config:\n",
    "        print(f\"\\nMissing Middleware Config:\")\n",
    "        for config in result.c_comp.missing_middleware_config:\n",
    "            print(f\"  ✗ {config}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I-ACC (Implementation Accuracy)\n",
      "============================================================\n",
      "Score: 100.0%\n",
      "\n",
      "Component Breakdown:\n",
      "  - File Location Correct: ✓\n",
      "  - Imports Correct: ✓\n",
      "  - Pattern Correct: ✓\n",
      "  - Placement Correct: ✓\n",
      "\n",
      "Weighting:\n",
      "  - File Location: 20% (+20.0%)\n",
      "  - Imports: 20% (+20.0%)\n",
      "  - Pattern: 30% (+30.0%)\n",
      "  - Placement: 30% (+30.0%)\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# I-ACC: Implementation Accuracy\n",
    "print(\"1. I-ACC (Implementation Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "if result.i_acc:\n",
    "    print(f\"Score: {result.i_acc.score:.1f}%\")\n",
    "    print(f\"\\nComponent Breakdown:\")\n",
    "    print(f\"  - File Location Correct: {'✓' if result.i_acc.file_location_correct else '✗'}\")\n",
    "    print(f\"  - Imports Correct: {'✓' if result.i_acc.imports_correct else '✗'}\")\n",
    "    print(f\"  - Pattern Correct: {'✓' if result.i_acc.pattern_correct else '✗'}\")\n",
    "    print(f\"  - Placement Correct: {'✓' if result.i_acc.placement_correct else '✗'}\")\n",
    "    \n",
    "    print(f\"\\nWeighting:\")\n",
    "    print(f\"  - File Location: 20% {'(+20.0%)' if result.i_acc.file_location_correct else '(0.0%)'}\")\n",
    "    print(f\"  - Imports: 20% {'(+20.0%)' if result.i_acc.imports_correct else '(0.0%)'}\")\n",
    "    print(f\"  - Pattern: 30% {'(+30.0%)' if result.i_acc.pattern_correct else '(0.0%)'}\")\n",
    "    print(f\"  - Placement: 30% {'(+30.0%)' if result.i_acc.placement_correct else '(0.0%)'}\")\n",
    "    \n",
    "    if result.i_acc.details:\n",
    "        print(\"\\nAdditional Details:\")\n",
    "        for key, value in result.i_acc.details.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. C-COMP (Configuration Completeness)\n",
      "============================================================\n",
      "Score: 0.0%\n",
      "\n",
      "Component Scores:\n",
      "  - Environment Variables: 0.0%\n",
      "  - Provider Properties: 0.0%\n",
      "  - Middleware Config: 0.0%\n",
      "\n",
      "Weighting:\n",
      "  - Environment Variables: 50% (contributes 0.0%)\n",
      "  - Provider Properties: 30% (contributes 0.0%)\n",
      "  - Middleware Config: 20% (contributes 0.0%)\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# C-COMP: Configuration Completeness  \n",
    "print(\"2. C-COMP (Configuration Completeness)\")\n",
    "print(\"=\"*60)\n",
    "if result.c_comp:\n",
    "    print(f\"Score: {result.c_comp.score:.1f}%\")\n",
    "    print(f\"\\nComponent Scores:\")\n",
    "    print(f\"  - Environment Variables: {result.c_comp.env_vars_score*100:.1f}%\")\n",
    "    print(f\"  - Provider Properties: {result.c_comp.provider_props_score*100:.1f}%\")\n",
    "    print(f\"  - Middleware Config: {result.c_comp.middleware_config_score*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nWeighting:\")\n",
    "    print(f\"  - Environment Variables: 50% (contributes {result.c_comp.env_vars_score*50:.1f}%)\")\n",
    "    print(f\"  - Provider Properties: 30% (contributes {result.c_comp.provider_props_score*30:.1f}%)\")\n",
    "    print(f\"  - Middleware Config: 20% (contributes {result.c_comp.middleware_config_score*20:.1f}%)\")\n",
    "    \n",
    "    if result.c_comp.missing_env_vars:\n",
    "        print(f\"\\nMissing Environment Variables:\")\n",
    "        for var in result.c_comp.missing_env_vars:\n",
    "            print(f\"  ✗ {var}\")\n",
    "    \n",
    "    if result.c_comp.missing_provider_props:\n",
    "        print(f\"\\nMissing Provider Properties:\")\n",
    "        for prop in result.c_comp.missing_provider_props:\n",
    "            print(f\"  ✗ {prop}\")\n",
    "            \n",
    "    if result.c_comp.missing_middleware_config:\n",
    "        print(f\"\\nMissing Middleware Config:\")\n",
    "        for config in result.c_comp.missing_middleware_config:\n",
    "            print(f\"  ✗ {config}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. IPA (Integration Point Accuracy)\n",
      "============================================================\n",
      "F1 Score: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "\n",
      "True Positives (Correct files): 0\n",
      "\n",
      "False Positives (Extra files): 0\n",
      "\n",
      "False Negatives (Missing files): 0\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# IPA: Integration Point Accuracy\n",
    "print(\"3. IPA (Integration Point Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "if result.ipa:\n",
    "    print(f\"F1 Score: {result.ipa.f1:.3f}\")\n",
    "    print(f\"Precision: {result.ipa.precision:.3f}\")\n",
    "    print(f\"Recall: {result.ipa.recall:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTrue Positives (Correct files): {len(result.ipa.true_positives)}\")\n",
    "    for file in result.ipa.true_positives[:5]:  # Show first 5\n",
    "        print(f\"  ✓ {file}\")\n",
    "    \n",
    "    print(f\"\\nFalse Positives (Extra files): {len(result.ipa.false_positives)}\")\n",
    "    for file in result.ipa.false_positives[:5]:\n",
    "        print(f\"  ✗ {file}\")\n",
    "    \n",
    "    print(f\"\\nFalse Negatives (Missing files): {len(result.ipa.false_negatives)}\")\n",
    "    for file in result.ipa.false_negatives[:5]:\n",
    "        print(f\"  ✗ {file}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. CQ (Code Quality)\n",
      "============================================================\n",
      "Score: 100.0%\n",
      "\n",
      "Issues Found:\n",
      "  - Type Errors: 0\n",
      "  - ESLint Errors: 0\n",
      "  - Security Issues: 0\n",
      "\n",
      "Score Calculation:\n",
      "  Base Score: 100%\n",
      "  Final Score: 100.0%\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CQ: Code Quality\n",
    "print(\"4. CQ (Code Quality)\")\n",
    "print(\"=\"*60)\n",
    "if result.cq:\n",
    "    print(f\"Score: {result.cq.score:.1f}%\")\n",
    "    print(f\"\\nIssues Found:\")\n",
    "    print(f\"  - Type Errors: {result.cq.type_errors}\")\n",
    "    print(f\"  - ESLint Errors: {result.cq.eslint_errors}\")\n",
    "    print(f\"  - Security Issues: {result.cq.security_issues}\")\n",
    "    \n",
    "    print(f\"\\nScore Calculation:\")\n",
    "    print(f\"  Base Score: 100%\")\n",
    "    if result.cq.type_errors > 0:\n",
    "        print(f\"  - Type Errors: -{result.cq.type_errors * 5}% ({result.cq.type_errors} × 5)\")\n",
    "    if result.cq.eslint_errors > 0:\n",
    "        print(f\"  - ESLint Errors: -{result.cq.eslint_errors * 2}% ({result.cq.eslint_errors} × 2)\")\n",
    "    if result.cq.security_issues > 0:\n",
    "        print(f\"  - Security Issues: -{result.cq.security_issues * 20}% ({result.cq.security_issues} × 20)\")\n",
    "    print(f\"  Final Score: {result.cq.score:.1f}%\")\n",
    "    \n",
    "    if result.cq.type_error_details:\n",
    "        print(\"\\nType Error Details:\")\n",
    "        for error in result.cq.type_error_details[:5]:  # Show first 5\n",
    "            print(f\"  - {error}\")\n",
    "            \n",
    "    if result.cq.eslint_error_details:\n",
    "        print(\"\\nESLint Error Details:\")\n",
    "        for error in result.cq.eslint_error_details[:5]:  # Show first 5\n",
    "            print(f\"  - {error}\")\n",
    "            \n",
    "    if result.cq.security_issue_details:\n",
    "        print(\"\\nSecurity Issue Details:\")\n",
    "        for issue in result.cq.security_issue_details[:5]:  # Show first 5\n",
    "            print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. SEM-SIM (Semantic Similarity)\n",
      "============================================================\n",
      "Score: 0.0%\n",
      "\n",
      "Similarity Score: 0.0%\n",
      "Pattern Match: ✗\n",
      "Approach Match: ✗\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SEM-SIM: Semantic Similarity\n",
    "print(\"5. SEM-SIM (Semantic Similarity)\")\n",
    "print(\"=\"*60)\n",
    "if result.sem_sim:\n",
    "    print(f\"Score: {result.sem_sim.score:.1f}%\")\n",
    "    print(f\"\\nSimilarity Score: {result.sem_sim.similarity_score:.1f}%\")\n",
    "    print(f\"Pattern Match: {'✓' if result.sem_sim.pattern_match else '✗'}\")\n",
    "    print(f\"Approach Match: {'✓' if result.sem_sim.approach_match else '✗'}\")\n",
    "    \n",
    "    if result.sem_sim.matched_patterns:\n",
    "        print(f\"\\nMatched Patterns ({len(result.sem_sim.matched_patterns)}):\")\n",
    "        for pattern in result.sem_sim.matched_patterns[:5]:  # Show first 5\n",
    "            print(f\"  ✓ {pattern}\")\n",
    "    \n",
    "    if result.sem_sim.missing_patterns:\n",
    "        print(f\"\\nMissing Patterns ({len(result.sem_sim.missing_patterns)}):\")\n",
    "        for pattern in result.sem_sim.missing_patterns[:5]:  # Show first 5\n",
    "            print(f\"  ✗ {pattern}\")\n",
    "            \n",
    "    if result.sem_sim.details:\n",
    "        print(\"\\nAdditional Details:\")\n",
    "        for key, value in result.sem_sim.details.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    print(\"Not evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Weighted Score Calculation\n",
    "\n",
    "The overall score is a weighted average of all metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Results:\n",
      "============================================================\n",
      "✓ No major issues found!\n",
      "\n",
      "Recommendations for Improvement:\n",
      "----------------------------------------\n",
      "1. Environment Configuration:\n",
      "   - Ensure .env or .env.local file is created\n",
      "   - Include all required API keys\n",
      "   - Use correct variable names (NEXT_PUBLIC_* for client-side)\n",
      "\n",
      "2. Code Structure:\n",
      "   - Follow framework conventions\n",
      "   - Use standard file naming\n",
      "   - Match expected component structure\n"
     ]
    }
   ],
   "source": [
    "# Analyze results to identify issues\n",
    "print(\"Analysis of Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check each metric\n",
    "if result.i_acc and result.i_acc.score < 100:\n",
    "    missing_components = []\n",
    "    if not result.i_acc.file_location_correct:\n",
    "        missing_components.append(\"file location\")\n",
    "    if not result.i_acc.imports_correct:\n",
    "        missing_components.append(\"imports\")\n",
    "    if not result.i_acc.pattern_correct:\n",
    "        missing_components.append(\"initialization pattern\")\n",
    "    if not result.i_acc.placement_correct:\n",
    "        missing_components.append(\"component placement\")\n",
    "    if missing_components:\n",
    "        issues.append(f\"I-ACC issues: {', '.join(missing_components)}\")\n",
    "\n",
    "if result.c_comp and result.c_comp.score < 100:\n",
    "    if result.c_comp.missing_env_vars:\n",
    "        issues.append(f\"Missing environment variables: {', '.join(result.c_comp.missing_env_vars)}\")\n",
    "    if result.c_comp.missing_provider_props:\n",
    "        issues.append(f\"Missing provider properties: {', '.join(result.c_comp.missing_provider_props)}\")\n",
    "    if result.c_comp.missing_middleware_config:\n",
    "        issues.append(f\"Missing middleware config: {', '.join(result.c_comp.missing_middleware_config)}\")\n",
    "\n",
    "if result.ipa and result.ipa.f1 < 1.0:\n",
    "    if result.ipa.false_negatives:\n",
    "        issues.append(f\"Missing integration in {len(result.ipa.false_negatives)} files\")\n",
    "    if result.ipa.false_positives:\n",
    "        issues.append(f\"Unnecessary changes in {len(result.ipa.false_positives)} files\")\n",
    "\n",
    "if result.cq and result.cq.score < 100:\n",
    "    quality_issues = []\n",
    "    if result.cq.type_errors > 0:\n",
    "        quality_issues.append(f\"{result.cq.type_errors} type errors\")\n",
    "    if result.cq.eslint_errors > 0:\n",
    "        quality_issues.append(f\"{result.cq.eslint_errors} linting errors\")\n",
    "    if result.cq.security_issues > 0:\n",
    "        quality_issues.append(f\"{result.cq.security_issues} security issues\")\n",
    "    if quality_issues:\n",
    "        issues.append(f\"Code quality issues: {', '.join(quality_issues)}\")\n",
    "\n",
    "if result.sem_sim and result.sem_sim.missing_patterns:\n",
    "    issues.append(f\"Missing {len(result.sem_sim.missing_patterns)} expected patterns\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Issues Found:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"{i}. {issue}\")\n",
    "else:\n",
    "    print(\"✓ No major issues found!\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRecommendations for Improvement:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if result.c_comp and result.c_comp.score == 0:\n",
    "    print(\"1. Environment Configuration:\")\n",
    "    print(\"   - Ensure .env or .env.local file is created\")\n",
    "    print(\"   - Include all required API keys\")\n",
    "    print(\"   - Use correct variable names (NEXT_PUBLIC_* for client-side)\")\n",
    "\n",
    "if result.sem_sim and result.sem_sim.score == 0:\n",
    "    print(\"\\n2. Code Structure:\")\n",
    "    print(\"   - Follow framework conventions\")\n",
    "    print(\"   - Use standard file naming\")\n",
    "    print(\"   - Match expected component structure\")\n",
    "\n",
    "if result.ipa and result.ipa.f1 < 0.8:\n",
    "    print(\"\\n3. Integration Points:\")\n",
    "    print(\"   - Review which files need SDK integration\")  \n",
    "    print(\"   - Avoid modifying unrelated files\")\n",
    "    print(\"   - Focus on the specific task requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Common Issues and Insights\n",
    "\n",
    "Based on the evaluation, let's identify common issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score Calculation:\n",
      "============================================================\n",
      "I-ACC       100.0% × 0.30 =  30.00%\n",
      "C-COMP        0.0% × 0.20 =   0.00%\n",
      "IPA         100.0% × 0.15 =  15.00%\n",
      "F-CORR        0.0% × 0.15 =   0.00%\n",
      "CQ          100.0% × 0.10 =  10.00%\n",
      "SEM-SIM       0.0% × 0.10 =   0.00%\n",
      "----------------------------------------\n",
      "Overall      55.0%\n",
      "\n",
      "Stored Overall Score: 33.5%\n"
     ]
    }
   ],
   "source": [
    "# Show how overall score is calculated\n",
    "print(\"Overall Score Calculation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weights = {\n",
    "    \"I-ACC\": 0.30,\n",
    "    \"C-COMP\": 0.20,\n",
    "    \"IPA\": 0.15,\n",
    "    \"F-CORR\": 0.15,\n",
    "    \"CQ\": 0.10,\n",
    "    \"SEM-SIM\": 0.10\n",
    "}\n",
    "\n",
    "scores = {\n",
    "    \"I-ACC\": result.i_acc.score if result.i_acc else 0,\n",
    "    \"C-COMP\": result.c_comp.score if result.c_comp else 0,\n",
    "    \"IPA\": result.ipa.f1 * 100 if result.ipa else 0,  # Convert to percentage\n",
    "    \"F-CORR\": result.f_corr.score if result.f_corr else 0,\n",
    "    \"CQ\": result.cq.score if result.cq else 0,\n",
    "    \"SEM-SIM\": result.sem_sim.score if result.sem_sim else 0\n",
    "}\n",
    "\n",
    "weighted_sum = 0\n",
    "for metric, weight in weights.items():\n",
    "    score = scores[metric]\n",
    "    contribution = score * weight\n",
    "    weighted_sum += contribution\n",
    "    print(f\"{metric:10} {score:6.1f}% × {weight:.2f} = {contribution:6.2f}%\")\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(f\"{'Overall':10} {weighted_sum:6.1f}%\")\n",
    "print(f\"\\nStored Overall Score: {result.overall_score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Results:\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CCompResult' object has no attribute 'missing_dependencies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.c_comp.missing_env_vars:\n\u001b[32m     13\u001b[39m         issues.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing environment variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.c_comp.missing_env_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_comp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmissing_dependencies\u001b[49m:\n\u001b[32m     15\u001b[39m         issues.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing dependencies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.c_comp.missing_dependencies\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.ipa \u001b[38;5;129;01mand\u001b[39;00m result.ipa.f1 < \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pydantic/main.py:892\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    891\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'CCompResult' object has no attribute 'missing_dependencies'"
     ]
    }
   ],
   "source": [
    "# Analyze results to identify issues\n",
    "print(\"Analysis of Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check each metric\n",
    "if result.i_acc and result.i_acc.score < 100:\n",
    "    issues.append(f\"Missing SDK features: {result.i_acc.features_missing}\")\n",
    "\n",
    "if result.c_comp and result.c_comp.score < 100:\n",
    "    if result.c_comp.missing_env_vars:\n",
    "        issues.append(f\"Missing environment variables: {result.c_comp.missing_env_vars}\")\n",
    "    if result.c_comp.missing_dependencies:\n",
    "        issues.append(f\"Missing dependencies: {result.c_comp.missing_dependencies}\")\n",
    "\n",
    "if result.ipa and result.ipa.f1 < 1.0:\n",
    "    if result.ipa.false_negatives:\n",
    "        issues.append(f\"Missing integration in {len(result.ipa.false_negatives)} files\")\n",
    "    if result.ipa.false_positives:\n",
    "        issues.append(f\"Unnecessary changes in {len(result.ipa.false_positives)} files\")\n",
    "\n",
    "if result.cq and result.cq.score < 100:\n",
    "    issues.append(f\"Code quality issues: {len(result.cq.issues if result.cq.issues else 0)} found\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Issues Found:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"{i}. {issue}\")\n",
    "else:\n",
    "    print(\"✓ No major issues found!\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRecommendations for Improvement:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if result.c_comp and result.c_comp.score == 0:\n",
    "    print(\"1. Environment Configuration:\")\n",
    "    print(\"   - Ensure .env or .env.local file is created\")\n",
    "    print(\"   - Include all required API keys\")\n",
    "    print(\"   - Use correct variable names (NEXT_PUBLIC_* for client-side)\")\n",
    "\n",
    "if result.sem_sim and result.sem_sim.score == 0:\n",
    "    print(\"\\n2. Code Structure:\")\n",
    "    print(\"   - Follow framework conventions\")\n",
    "    print(\"   - Use standard file naming\")\n",
    "    print(\"   - Match expected component structure\")\n",
    "\n",
    "if result.ipa and result.ipa.f1 < 0.8:\n",
    "    print(\"\\n3. Integration Points:\")\n",
    "    print(\"   - Review which files need SDK integration\")  \n",
    "    print(\"   - Avoid modifying unrelated files\")\n",
    "    print(\"   - Focus on the specific task requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    \"sample_id\": SAMPLE_ID,\n",
    "    \"task_type\": task_types[current_task_type]['name'],\n",
    "    \"overall_score\": f\"{result.overall_score:.1f}%\",\n",
    "    \"metrics\": {\n",
    "        \"I-ACC (Implementation)\": f\"{scores['I-ACC']:.1f}%\",\n",
    "        \"C-COMP (Configuration)\": f\"{scores['C-COMP']:.1f}%\",\n",
    "        \"IPA (Integration Points)\": f\"{scores['IPA']:.1f}%\",\n",
    "        \"F-CORR (Functional)\": f\"{scores['F-CORR']:.1f}%\",\n",
    "        \"CQ (Code Quality)\": f\"{scores['CQ']:.1f}%\",\n",
    "        \"SEM-SIM (Similarity)\": f\"{scores['SEM-SIM']:.1f}%\"\n",
    "    },\n",
    "    \"key_strengths\": [],\n",
    "    \"key_weaknesses\": []\n",
    "}\n",
    "\n",
    "# Identify strengths and weaknesses\n",
    "for metric, score in scores.items():\n",
    "    if score >= 80:\n",
    "        summary[\"key_strengths\"].append(f\"{metric}: {score:.1f}%\")\n",
    "    elif score < 50:\n",
    "        summary[\"key_weaknesses\"].append(f\"{metric}: {score:.1f}%\")\n",
    "\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Performance interpretation\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "print(\"-\"*40)\n",
    "if result.overall_score >= 80:\n",
    "    print(\"🏆 Excellent: The LLM successfully implemented the SDK task\")\n",
    "elif result.overall_score >= 60:\n",
    "    print(\"✅ Good: The core functionality is correct with minor issues\")\n",
    "elif result.overall_score >= 40:\n",
    "    print(\"⚠️ Fair: Basic implementation present but significant gaps\")\n",
    "else:\n",
    "    print(\"❌ Poor: Major issues in SDK implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Testing Different Models\n",
    "\n",
    "You can test different models by changing the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models and their characteristics\n",
    "models = {\n",
    "    \"Anthropic\": [\n",
    "        {\n",
    "            \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "            \"description\": \"Most capable, best for complex tasks\",\n",
    "            \"cost\": \"$3/$15 per 1M tokens (input/output)\",\n",
    "            \"speed\": \"Medium\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"claude-3-haiku-20240307\",\n",
    "            \"description\": \"Fast and affordable, good for simple tasks\",\n",
    "            \"cost\": \"$0.25/$1.25 per 1M tokens\",\n",
    "            \"speed\": \"Very Fast\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"claude-haiku-4-5-20251001\",\n",
    "            \"description\": \"Claude Haiku 4.5 - newer version\",\n",
    "            \"cost\": \"$1/$5 per 1M tokens\",\n",
    "            \"speed\": \"Fast\"\n",
    "        }\n",
    "    ],\n",
    "    \"OpenAI\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4-turbo-preview\",\n",
    "            \"description\": \"GPT-4 Turbo with latest improvements\",\n",
    "            \"cost\": \"$10/$30 per 1M tokens\",\n",
    "            \"speed\": \"Medium\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"description\": \"Fast and cheap, good baseline\",\n",
    "            \"cost\": \"$0.50/$1.50 per 1M tokens\",\n",
    "            \"speed\": \"Very Fast\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Available Models for Testing:\")\n",
    "print(\"=\"*60)\n",
    "for provider, provider_models in models.items():\n",
    "    print(f\"\\n{provider}:\")\n",
    "    for model_info in provider_models:\n",
    "        print(f\"  • {model_info['model']}\")\n",
    "        print(f\"    - {model_info['description']}\")\n",
    "        print(f\"    - Cost: {model_info['cost']}\")\n",
    "        print(f\"    - Speed: {model_info['speed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try different samples**: Change `SAMPLE_ID` to test other task types\n",
    "2. **Generate new solutions**: Set up API key and generate fresh solutions  \n",
    "3. **Compare models**: Test different LLMs on the same task\n",
    "4. **Analyze patterns**: Look for common failure modes across tasks\n",
    "5. **Improve prompts**: Based on the evaluation results, refine prompts\n",
    "\n",
    "This notebook provides a complete understanding of how SDK-Bench evaluates LLM capabilities for SDK instrumentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdkbench.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(solution_dir, metadata_path=metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['package.json', 'generation_metadata.json', 'app/layout.tsx', 'app/page.tsx'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.i_acc_evaluator.solution.files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ingredients': {'initialization': {'location': 'app/layout.tsx',\n",
       "   'pattern': 'ClerkProvider wrapper',\n",
       "   'imports': ['@clerk/nextjs']},\n",
       "  'configuration': {'env_vars': ['NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY',\n",
       "    'CLERK_SECRET_KEY'],\n",
       "   'provider_props': [],\n",
       "   'optional_config': []}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.i_acc_evaluator.ground_truth.ground_truth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
