{
  "score": 0.0,
  "tests_passed": 2,
  "tests_failed": 1,
  "tests_total": 3,
  "tests_skipped": 0,
  "duration": 7.6704089641571045,
  "error": "1 tests failed",
  "failed_tests": [
    "test_pipeline_end_to_end"
  ],
  "failure_details": [
    {
      "test_name": "test_pipeline_end_to_end",
      "file_path": "tests/test_pipeline.py",
      "line_number": 38,
      "error_message": "assert False",
      "stack_trace": "tests/test_pipeline.py:38: in test_pipeline_end_to_end\n    assert isinstance(response, str)\nE   assert False\nE    +  where False = isinstance({'num_results': 3, 'query': 'test query', 'response': \"Based on the retrieved context, here's the answer to your query:\\n\\nQuery: test query\\n\\nRetrieved Context:\\nDocument 1 (Score: 8.5328):\\nTest document\\n\\nDocument 2 (Score: 30.1309):\\nTest content for pipeline\\n\\nDocument 3 (Score: 45.9315):\\nLanceDB is great\\n\\nAnswer: [This is a mock response. In production, an LLM would generate a comprehensive answer based on the context above.]\\n\\nThe most relevant document has a similarity score of 8.5328, indicating moderate relevance to your query.\\n\", 'retrieved_documents': [{'id': 'doc_0', 'metadata': {}, 'score': 8.532835006713867, 'text': 'Test document'}, {'id': 'doc_0', 'metadata': {}, 'score': 30.130861282348633, 'text': 'Test content for pipeline'}, {'id': 'doc_0', 'metadata': {}, 'score': 45.93154525756836, 'text': 'LanceDB is great'}]}, str)"
    }
  ],
  "raw_output": "============================= test session starts ==============================\nplatform darwin -- Python 3.12.0, pytest-9.0.1, pluggy-1.6.0 -- /Users/arshath/miniforge3/bin/python\ncachedir: .pytest_cache\nrootdir: /private/var/folders/wx/bj4n9pyj10n74txq4mpt0j9h0000gn/T/fcorr_xt8y60kf\nplugins: anyio-4.7.0\ncollecting ... collected 3 items\n\ntests/test_pipeline.py::test_document_ingestion PASSED                   [ 33%]\ntests/test_pipeline.py::test_search_returns_results PASSED               [ 66%]\ntests/test_pipeline.py::test_pipeline_end_to_end FAILED                  [100%]\n\n=================================== FAILURES ===================================\n___________________________ test_pipeline_end_to_end ___________________________\ntests/test_pipeline.py:38: in test_pipeline_end_to_end\n    assert isinstance(response, str)\nE   assert False\nE    +  where False = isinstance({'num_results': 3, 'query': 'test query', 'response': \"Based on the retrieved context, here's the answer to your query:\\n\\nQuery: test query\\n\\nRetrieved Context:\\nDocument 1 (Score: 8.5328):\\nTest document\\n\\nDocument 2 (Score: 30.1309):\\nTest content for pipeline\\n\\nDocument 3 (Score: 45.9315):\\nLanceDB is great\\n\\nAnswer: [This is a mock response. In production, an LLM would generate a comprehensive answer based on the context above.]\\n\\nThe most relevant document has a similarity score of 8.5328, indicating moderate relevance to your query.\\n\", 'retrieved_documents': [{'id': 'doc_0', 'metadata': {}, 'score': 8.532835006713867, 'text': 'Test document'}, {'id': 'doc_0', 'metadata': {}, 'score': 30.130861282348633, 'text': 'Test content for pipeline'}, {'id': 'doc_0', 'metadata': {}, 'score': 45.93154525756836, 'text': 'LanceDB is great'}]}, str)\n=========================== short test summary info ============================\nFAILED tests/test_pipeline.py::test_pipeline_end_to_end - assert False\n========================= 1 failed, 2 passed in 6.25s ==========================\n"
}