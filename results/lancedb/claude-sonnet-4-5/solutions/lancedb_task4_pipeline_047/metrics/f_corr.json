{
  "score": 0.0,
  "tests_passed": 2,
  "tests_failed": 1,
  "tests_total": 3,
  "tests_skipped": 0,
  "duration": 8.50694489479065,
  "error": "1 tests failed",
  "failed_tests": [
    "test_pipeline_end_to_end"
  ],
  "failure_details": [
    {
      "test_name": "test_pipeline_end_to_end",
      "file_path": "tests/test_pipeline.py",
      "line_number": 38,
      "error_message": "assert False",
      "stack_trace": "tests/test_pipeline.py:38: in test_pipeline_end_to_end\n    assert isinstance(response, str)\nE   assert False\nE    +  where False = isinstance({'context': [{'id': 0, 'score': 0.9788000583648682, 'text': 'Test document'}, {'id': 0, 'score': 1.389192819595337, 'text': 'Test content for pipeline'}, {'id': 0, 'score': 1.8318662643432617, 'text': 'LanceDB is great'}], 'num_results': 3, 'query': 'test query', 'response': \"Based on the retrieved context, here's the answer to your query:\\n\\nQuery: test query\\n\\nRetrieved Context:\\n[Document 1] Test document\\n\\n[Document 2] Test content for pipeline\\n\\n[Document 3] LanceDB is great\\n\\nAnswer: Based on the 3 most relevant documents, Test document\\n\\n(Note: This is a mock response. In production, this would be generated by an LLM like GPT-4, Claude, or Llama.)\\n\"}, str)\n----------------------------- Captured stdout call -----------------------------\nGenerating embeddings for 1 documents...\nAdded 1 documents to existing table\nCreated full-text search index\n\n============================================================\nRunning RAG Pipeline\n============================================================\n\nStep 1: Retrieving relevant documents...\nPerforming vector search for: 'test query'\nFound 3 results\n\nStep 2: Generating response..."
    }
  ],
  "raw_output": "============================= test session starts ==============================\nplatform darwin -- Python 3.12.0, pytest-9.0.1, pluggy-1.6.0 -- /Users/arshath/miniforge3/bin/python\ncachedir: .pytest_cache\nrootdir: /private/var/folders/wx/bj4n9pyj10n74txq4mpt0j9h0000gn/T/fcorr_6yxvldpk\nplugins: anyio-4.7.0\ncollecting ... collected 3 items\n\ntests/test_pipeline.py::test_document_ingestion PASSED                   [ 33%]\ntests/test_pipeline.py::test_search_returns_results PASSED               [ 66%]\ntests/test_pipeline.py::test_pipeline_end_to_end FAILED                  [100%]\n\n=================================== FAILURES ===================================\n___________________________ test_pipeline_end_to_end ___________________________\ntests/test_pipeline.py:38: in test_pipeline_end_to_end\n    assert isinstance(response, str)\nE   assert False\nE    +  where False = isinstance({'context': [{'id': 0, 'score': 0.9788000583648682, 'text': 'Test document'}, {'id': 0, 'score': 1.389192819595337, 'text': 'Test content for pipeline'}, {'id': 0, 'score': 1.8318662643432617, 'text': 'LanceDB is great'}], 'num_results': 3, 'query': 'test query', 'response': \"Based on the retrieved context, here's the answer to your query:\\n\\nQuery: test query\\n\\nRetrieved Context:\\n[Document 1] Test document\\n\\n[Document 2] Test content for pipeline\\n\\n[Document 3] LanceDB is great\\n\\nAnswer: Based on the 3 most relevant documents, Test document\\n\\n(Note: This is a mock response. In production, this would be generated by an LLM like GPT-4, Claude, or Llama.)\\n\"}, str)\n----------------------------- Captured stdout call -----------------------------\nGenerating embeddings for 1 documents...\nAdded 1 documents to existing table\nCreated full-text search index\n\n============================================================\nRunning RAG Pipeline\n============================================================\n\nStep 1: Retrieving relevant documents...\nPerforming vector search for: 'test query'\nFound 3 results\n\nStep 2: Generating response...\n=========================== short test summary info ============================\nFAILED tests/test_pipeline.py::test_pipeline_end_to_end - assert False\n========================= 1 failed, 2 passed in 7.13s ==========================\n"
}